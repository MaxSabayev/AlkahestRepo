{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we applied the Elastic Net model on the proteomics data to predict the treatment groups (Placabo vs. GRF6021) at each time point V3, V4a, V4b and V5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elastic Net Prediction at Each Timepoint (V3, V4a, V4b, V5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import Series,DataFrame\n",
    "import canopy\n",
    "#EN model\n",
    "from statannot import add_stat_annotation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#k-means and pathway analysis\n",
    "from sklearn.manifold import TSNE\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.cluster import KMeans\n",
    "import colorcet as cc\n",
    "import gseapy as gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the data for EN model\n",
    "\n",
    "def process_time_point(df, time_point):\n",
    "    # Filter the DataFrame for the given time point and remove the first column\n",
    "    df_withid = df[df['Time point'] == time_point].iloc[:, 1:]\n",
    "    df_withid.reset_index(inplace=True,drop=True)\n",
    "    # Extract features and apply log2 transformation\n",
    "    X_df = df_withid.iloc[:, 2:]\n",
    "    X_df = np.log2(X_df)\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_df_stan = pd.DataFrame(scaler.fit_transform(X_df), index=X_df.index, columns=X_df.columns)\n",
    "    \n",
    "    # Map the 'Treatment' column to numerical values\n",
    "    df_withid['Treatment'] = df_withid['Treatment'].map({'GRF6021': 1, 'Placebo': 2})\n",
    "    \n",
    "    # Extract the target variable\n",
    "    y_pro = df_withid.iloc[:, 1]\n",
    "    \n",
    "    return X_df_stan, y_pro\n",
    "\n",
    "# Time points to process\n",
    "time_points = ['V3', 'V4a', 'V4b', 'V5']\n",
    "\n",
    "# Dictionaries to hold the processed features and targets\n",
    "X_pro_stan = {}\n",
    "y_pro = {}\n",
    "\n",
    "# Process each time point\n",
    "#read the data\n",
    "all_pro=pd.read_excel(\"Proteomics_all.xlsx\")\n",
    "for tp in time_points:\n",
    "    X_pro_stan[tp], y_pro[tp] = process_time_point(all_pro, tp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def make_predictions(X,y):\n",
    "    # X: assume X is a dataframe of training data\n",
    "    # y: y is the label of the data\n",
    "    # \n",
    "    # return a dataframe of the ans \n",
    "    #\n",
    "    regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "    X.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X.columns.values]\n",
    "    rows = max(X.index) + 1\n",
    "    ans_matrix = []\n",
    "    auc_roc = []\n",
    "    feature_importance={}\n",
    "    #Iterate 100 experiments, in each experiment, we randomly select 50% of the data as training data and the rest 50% as testing data.\n",
    "    for i in range(100):\n",
    "        # a ans_list is a single column in the orignial ans matrix\n",
    "        ans_list = [None] * rows\n",
    "        df_train = X.sample(frac=0.5)\n",
    "        df_test = X.drop(df_train.index)\n",
    "        # initiate an empty model   \n",
    "        # define model\n",
    "        reg = LogisticRegression(penalty = 'elasticnet', solver = 'saga', l1_ratio = 0.9)\n",
    "        # train the model        \n",
    "        reg.fit(df_train, y[df_train.index])\n",
    "        # use the trained model to make predictions\n",
    "        df_test_label = reg.predict(df_test)\n",
    "        importance=reg.coef_[0]\n",
    "        # summarize feature importance\n",
    "        \n",
    "        for i,v in enumerate(importance):\n",
    "            if i not in feature_importance:\n",
    "                feature_importance[i]=[]\n",
    "            feature_importance[i].append(v)\n",
    "        # assign the predicted results to the ans_list\n",
    "        start = 0\n",
    "        for j in df_test.index:\n",
    "            ans_list[j] = df_test_label[start]\n",
    "            start += 1\n",
    "            \n",
    "        ans_matrix.append(ans_list)\n",
    "    \n",
    "    ans = np.array(ans_matrix)\n",
    "    ans = np.transpose(ans)\n",
    "    df_ans = pd.DataFrame(ans)\n",
    "    \n",
    "    return df_ans,feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store the predictions, feature importances, and result DataFrames\n",
    "ans = {}\n",
    "fi = {}\n",
    "df_result = {}\n",
    "df_withid = {}\n",
    "\n",
    "# Loop through each time point, make predictions, and prepare the result DataFrame\n",
    "time_points=['V3','V4a','V4b','V5']\n",
    "\n",
    "for tp in time_points:\n",
    "    \n",
    "    # Making predictions\n",
    "    ans[tp], fi[tp] = make_predictions(X_pro_stan[tp], y_pro[tp])\n",
    "    ans[tp]['mean'] = ans[tp].mean(axis=1) \n",
    "    \n",
    "    # Concatenating the 'Treatment' and 'mean' data\n",
    "    df_withid[tp] = all_pro[all_pro['Time point'] == tp]\n",
    "    df_withid[tp].reset_index(inplace=True,drop=True)\n",
    "    df_result[tp] = pd.concat([df_withid[tp]['Treatment'], ans[tp]['mean']], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize the prediction results\n",
    "\n",
    "fig, axs = plt.subplots(2, 2,figsize=(20,20))\n",
    "fig.suptitle('Elastic Net Model of Proteomics Data Prediction Results',fontsize=25)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "#V3\n",
    "sns.boxplot(ax=axs[0,0],x=\"Treatment\",y=\"mean\",data=df_result['V3'],order=[\"GRF6021\",\"Placebo\"])\n",
    "axs[0, 0].set_title('V3',fontsize=32)\n",
    "axs[0, 0].set_ylim(0.8,2.4)\n",
    "axs[0, 0].set_xlabel('Treatment', fontsize=20)\n",
    "axs[0, 0].tick_params(axis='both', labelsize=20)\n",
    "axs[0, 0].set_ylabel('Predicted value', fontsize=20)\n",
    "add_stat_annotation(ax=axs[0,0], data=df_result_V3, x=\"Treatment\", y=\"mean\",\n",
    "                    box_pairs=[(\"GRF6021\", \"Placebo\")],\n",
    "                    test='t-test_ind', text_format='simple', loc='inside', verbose=2)\n",
    "\n",
    "#V4a\n",
    "sns.boxplot(ax=axs[0,1],x=\"Treatment\",y=\"mean\",data=df_result['V4a'],order=[\"GRF6021\",\"Placebo\"])\n",
    "axs[0, 1].set_title('V4a',fontsize=32)\n",
    "axs[0, 1].set_ylim(0.8,2.4)\n",
    "axs[0, 1].set_xlabel('Treatment', fontsize=20)\n",
    "axs[0, 1].tick_params(axis='both', labelsize=20)\n",
    "axs[0, 1].set_ylabel('Predicted value', fontsize=20)\n",
    "add_stat_annotation(ax=axs[0,1], data=df_result_V4a, x=\"Treatment\", y=\"mean\",\n",
    "                    box_pairs=[(\"GRF6021\", \"Placebo\")],\n",
    "                    test='t-test_ind', text_format='simple', loc='inside', verbose=2)\n",
    "#V4b\n",
    "sns.boxplot(ax=axs[1,0],x=\"Treatment\",y=\"mean\",data=df_result_V4b,order=[\"GRF6021\",\"Placebo\"])\n",
    "axs[1, 0].set_title('V4b',fontsize=32)\n",
    "axs[1, 0].set_ylim(0.8,2.4)\n",
    "axs[1, 0].set_xlabel('Treatment', fontsize=20)\n",
    "axs[1, 0].tick_params(axis='both', labelsize=20)\n",
    "axs[1, 0].set_ylabel('Predicted value', fontsize=20)\n",
    "add_stat_annotation(ax=axs[1,0], data=df_result['V4b'], x=\"Treatment\", y=\"mean\",\n",
    "                    box_pairs=[(\"GRF6021\", \"Placebo\")],\n",
    "                    test='t-test_ind', text_format='simple', loc='inside', verbose=2)\n",
    "#V5\n",
    "sns.boxplot(ax=axs[1,1],x=\"Treatment\",y=\"mean\",data=df_result['V5'],order=[\"GRF6021\",\"Placebo\"])\n",
    "axs[1, 1].set_title('V5',fontsize=32)\n",
    "axs[1, 1].set_ylim(0.8,2.4)\n",
    "axs[1, 1].set_xlabel('Treatment', fontsize=20)\n",
    "axs[1, 1].tick_params(axis='both', labelsize=20)\n",
    "axs[1, 1].set_ylabel('Predicted value', fontsize=20)\n",
    "add_stat_annotation(ax=axs[1,1], data=df_result_V5, x=\"Treatment\", y=\"mean\",\n",
    "                    box_pairs=[(\"GRF6021\", \"Placebo\")],\n",
    "                    test='t-test_ind', text_format='simple', loc='inside', verbose=2)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(ylabel='Prediction value')\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
