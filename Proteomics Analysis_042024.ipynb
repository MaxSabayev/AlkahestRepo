{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we applied the Elastic Net model on the proteomics data to predict the treatment groups (Placabo vs. GRF6021) at each time point V3, V4a, V4b and V5. Then we utilized k-means to cluster the proteins and applied the over-representation pathway analysis to annotate each cluster at V3 timepoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.Elastic Net Prediction at Each Timepoint (V3, V4a, V4b, V5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import Series,DataFrame\n",
    "import canopy\n",
    "#EN model\n",
    "from statannot import add_stat_annotation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#k-means and pathway analysis\n",
    "from sklearn.manifold import TSNE\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.cluster import KMeans\n",
    "import colorcet as cc\n",
    "import gseapy as gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the data for EN model\n",
    "\n",
    "def process_time_point(df, time_point):\n",
    "    # Filter the DataFrame for the given time point and remove the first column\n",
    "    df_withid = df[df['Time point'] == time_point].iloc[:, 1:]\n",
    "    df_withid.reset_index(inplace=True,drop=True)\n",
    "    # Extract features and apply log2 transformation\n",
    "    X_df = df_withid.iloc[:, 2:]\n",
    "    X_df = np.log2(X_df)\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_df_stan = pd.DataFrame(scaler.fit_transform(X_df), index=X_df.index, columns=X_df.columns)\n",
    "    \n",
    "    # Map the 'Treatment' column to numerical values\n",
    "    df_withid['Treatment'] = df_withid['Treatment'].map({'GRF6021': 1, 'Placebo': 2})\n",
    "    \n",
    "    # Extract the target variable\n",
    "    y_pro = df_withid.iloc[:, 1]\n",
    "    \n",
    "    return X_df_stan, y_pro\n",
    "\n",
    "# Time points to process\n",
    "time_points = ['V3', 'V4a', 'V4b', 'V5']\n",
    "\n",
    "# Dictionaries to hold the processed features and targets\n",
    "X_pro_stan = {}\n",
    "y_pro = {}\n",
    "\n",
    "# Process each time point\n",
    "#read the data\n",
    "all_pro=pd.read_excel(\"Proteomics_all.xlsx\")\n",
    "for tp in time_points:\n",
    "    X_pro_stan[tp], y_pro[tp] = process_time_point(all_pro, tp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def make_predictions(X,y):\n",
    "    # X: assume X is a dataframe of training data\n",
    "    # y: y is the label of the data\n",
    "    # \n",
    "    # return a dataframe of the ans \n",
    "    #\n",
    "    regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "    X.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X.columns.values]\n",
    "    rows = max(X.index) + 1\n",
    "    ans_matrix = []\n",
    "    auc_roc = []\n",
    "    feature_importance={}\n",
    "    #Iterate 100 experiments, in each experiment, we randomly select 50% of the data as training data and the rest 50% as testing data.\n",
    "    for i in range(100):\n",
    "        # a ans_list is a single column in the orignial ans matrix\n",
    "        ans_list = [None] * rows\n",
    "        df_train = X.sample(frac=0.5)\n",
    "        df_test = X.drop(df_train.index)\n",
    "        # initiate an empty model   \n",
    "        # define model\n",
    "        reg = LogisticRegression(penalty = 'elasticnet', solver = 'saga', l1_ratio = 0.9)\n",
    "        # train the model        \n",
    "        reg.fit(df_train, y[df_train.index])\n",
    "        # use the trained model to make predictions\n",
    "        df_test_label = reg.predict(df_test)\n",
    "        importance=reg.coef_[0]\n",
    "        # summarize feature importance\n",
    "        \n",
    "        for i,v in enumerate(importance):\n",
    "            if i not in feature_importance:\n",
    "                feature_importance[i]=[]\n",
    "            feature_importance[i].append(v)\n",
    "        # assign the predicted results to the ans_list\n",
    "        start = 0\n",
    "        for j in df_test.index:\n",
    "            ans_list[j] = df_test_label[start]\n",
    "            start += 1\n",
    "            \n",
    "        ans_matrix.append(ans_list)\n",
    "    \n",
    "    ans = np.array(ans_matrix)\n",
    "    ans = np.transpose(ans)\n",
    "    df_ans = pd.DataFrame(ans)\n",
    "    \n",
    "    return df_ans,feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store the predictions, feature importances, and result DataFrames\n",
    "ans = {}\n",
    "fi = {}\n",
    "df_result = {}\n",
    "df_withid = {}\n",
    "\n",
    "# Loop through each time point, make predictions, and prepare the result DataFrame\n",
    "time_points=['V3','V4a','V4b','V5']\n",
    "\n",
    "for tp in time_points:\n",
    "    \n",
    "    # Making predictions\n",
    "    ans[tp], fi[tp] = make_predictions(X_pro_stan[tp], y_pro[tp])\n",
    "    ans[tp]['mean'] = ans[tp].mean(axis=1) \n",
    "    \n",
    "    # Concatenating the 'Treatment' and 'mean' data\n",
    "    df_withid[tp] = all_pro[all_pro['Time point'] == tp]\n",
    "    df_withid[tp].reset_index(inplace=True,drop=True)\n",
    "    df_result[tp] = pd.concat([df_withid[tp]['Treatment'], ans[tp]['mean']], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize the prediction results\n",
    "\n",
    "fig, axs = plt.subplots(2, 2,figsize=(20,20))\n",
    "fig.suptitle('Elastic Net Model of Proteomics(7272) Data Prediction Results',fontsize=25)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "#V3\n",
    "sns.boxplot(ax=axs[0,0],x=\"Treatment\",y=\"mean\",data=df_result['V3'],order=[\"GRF6021\",\"Placebo\"])\n",
    "axs[0, 0].set_title('V3',fontsize=32)\n",
    "axs[0, 0].set_ylim(0.8,2.4)\n",
    "axs[0, 0].set_xlabel('Treatment', fontsize=20)\n",
    "axs[0, 0].tick_params(axis='both', labelsize=20)\n",
    "axs[0, 0].set_ylabel('Predicted value', fontsize=20)\n",
    "add_stat_annotation(ax=axs[0,0], data=df_result_V3, x=\"Treatment\", y=\"mean\",\n",
    "                    box_pairs=[(\"GRF6021\", \"Placebo\")],\n",
    "                    test='t-test_ind', text_format='simple', loc='inside', verbose=2)\n",
    "\n",
    "#V4a\n",
    "sns.boxplot(ax=axs[0,1],x=\"Treatment\",y=\"mean\",data=df_result['V4a'],order=[\"GRF6021\",\"Placebo\"])\n",
    "axs[0, 1].set_title('V4a',fontsize=32)\n",
    "axs[0, 1].set_ylim(0.8,2.4)\n",
    "axs[0, 1].set_xlabel('Treatment', fontsize=20)\n",
    "axs[0, 1].tick_params(axis='both', labelsize=20)\n",
    "axs[0, 1].set_ylabel('Predicted value', fontsize=20)\n",
    "add_stat_annotation(ax=axs[0,1], data=df_result_V4a, x=\"Treatment\", y=\"mean\",\n",
    "                    box_pairs=[(\"GRF6021\", \"Placebo\")],\n",
    "                    test='t-test_ind', text_format='simple', loc='inside', verbose=2)\n",
    "#V4b\n",
    "sns.boxplot(ax=axs[1,0],x=\"Treatment\",y=\"mean\",data=df_result_V4b,order=[\"GRF6021\",\"Placebo\"])\n",
    "axs[1, 0].set_title('V4b',fontsize=32)\n",
    "axs[1, 0].set_ylim(0.8,2.4)\n",
    "axs[1, 0].set_xlabel('Treatment', fontsize=20)\n",
    "axs[1, 0].tick_params(axis='both', labelsize=20)\n",
    "axs[1, 0].set_ylabel('Predicted value', fontsize=20)\n",
    "add_stat_annotation(ax=axs[1,0], data=df_result['V4b'], x=\"Treatment\", y=\"mean\",\n",
    "                    box_pairs=[(\"GRF6021\", \"Placebo\")],\n",
    "                    test='t-test_ind', text_format='simple', loc='inside', verbose=2)\n",
    "#V5\n",
    "sns.boxplot(ax=axs[1,1],x=\"Treatment\",y=\"mean\",data=df_result['V5'],order=[\"GRF6021\",\"Placebo\"])\n",
    "axs[1, 1].set_title('V5',fontsize=32)\n",
    "axs[1, 1].set_ylim(0.8,2.4)\n",
    "axs[1, 1].set_xlabel('Treatment', fontsize=20)\n",
    "axs[1, 1].tick_params(axis='both', labelsize=20)\n",
    "axs[1, 1].set_ylabel('Predicted value', fontsize=20)\n",
    "add_stat_annotation(ax=axs[1,1], data=df_result_V5, x=\"Treatment\", y=\"mean\",\n",
    "                    box_pairs=[(\"GRF6021\", \"Placebo\")],\n",
    "                    test='t-test_ind', text_format='simple', loc='inside', verbose=2)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(ylabel='Prediction value')\n",
    "    \n",
    "plt.savefig('Proteomics_remove1_boxplot_7272.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. K-means Clustering with Top Pathways**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract V3 data\n",
    "df_V3=df_withid['V3']\n",
    "df_V3_feature=df_V3.iloc[:,3:]\n",
    "#target column\n",
    "df_V3_target=df_V3.iloc[:,2]\n",
    "df_V3_all=pd.concat([df_V3_feature,df_V3_target],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caculate the correlation matrix of all significant features\n",
    "correlation_matrix = df_V3_feature.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Tsne to layout the nodes\n",
    "x = df_V3_feature.T\n",
    "tsne = TSNE(n_components=2, random_state=123)\n",
    "z = tsne.fit_transform(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the plot\n",
    "df = pd.DataFrame()\n",
    "df[\"comp-1\"] = z[:,0]\n",
    "df[\"comp-2\"] = z[:,1]\n",
    "\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\",\n",
    "                palette=sns.color_palette(\"hls\", 3),\n",
    "                data=df).set(title=\"V3 T-SNE projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the position of each node same as the Tsne\n",
    "my_pos={}\n",
    "for idx,i in enumerate(list(correlation_matrix.index)):\n",
    "    my_pos[i]=tuple((z)[idx].tolist())\n",
    "print(my_pos)\n",
    "print(len(my_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe to store the position \n",
    "my_pos_df=pd.DataFrame.from_dict(my_pos).T\n",
    "my_pos_df.columns=['X','Y']\n",
    "my_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilize networkx to generate the plot\n",
    "node_list=list(correlation_matrix.columns)\n",
    "#node_list=short_name_list\n",
    "# initialize nodes with node properties\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(my_pos.keys(),size=10)\n",
    "for n, p in my_pos.items():\n",
    "    G.nodes[n]['pos'] = p\n",
    "nx.draw(G,pos=my_pos,node_color='lightgray',node_size=50,with_labels=True,font_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_x = []\n",
    "node_y = []\n",
    "for node in G.nodes():\n",
    "    x, y = G.nodes[node]['pos']#record the position\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    marker=dict(\n",
    "        showscale=False,\n",
    "        reversescale=True,\n",
    "        color='grey',\n",
    "        size=10,\n",
    "        line_width=[]))\n",
    "G.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_text = []\n",
    "for node in G.nodes:\n",
    "    node_text.append(node)\n",
    "\n",
    "node_trace.text = node_text\n",
    "fig = go.Figure(data=[node_trace],\n",
    "             layout=go.Layout(\n",
    "                title='<br>V3 Tsne',\n",
    "                titlefont_size=16,\n",
    "                showlegend=False,\n",
    "                #hovermode='closest',\n",
    "                margin=dict(b=20,l=5,r=5,t=40),\n",
    "                xaxis=dict(showgrid=True, zeroline=False, showticklabels=True),\n",
    "                yaxis=dict(showgrid=True, zeroline=False, showticklabels=True))\n",
    "                )\n",
    "fig.show()\n",
    "fig.write_html(\"Interactive_tsne.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-means\n",
    "x=node_x\n",
    "y=node_y\n",
    "#use Elbow method to derermine the number of clusters\n",
    "data = list(zip(x, y))\n",
    "inertias = []\n",
    "\n",
    "for i in range(1,30):\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    kmeans.fit(data)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,30), inertias, marker='o')\n",
    "plt.title('Elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decide to use 20 as the number of clusters\n",
    "kmeans = KMeans(n_clusters=20,random_state=66)\n",
    "kmeans.fit(z)\n",
    "plt.scatter(x, y, c=kmeans.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "\n",
    "# Get cluster centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(z[:, 0], z[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=200, color='red')\n",
    "\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.title('K-means Clustering in t-SNE Space')\n",
    "#plt.legend(*sc.legend_elements(), title='clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(z[:, 0], z[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "#plt.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=200)\n",
    "\n",
    "for i,(center_x,center_y) in enumerate(centroids):\n",
    "    plt.text(center_x, center_y, f'Clu_{i}', fontsize=10,color='red', ha='center', va='center')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.title('K-means Clustering in t-SNE Space')\n",
    "#plt.legend(*sc.legend_elements(), title='clusters')\n",
    "plt.savefig('V3_clustering_id.png',bbox_inches=\"tight\",dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pos_df['class']=labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "palette = sns.color_palette(cc.glasbey_light, n_colors=20)\n",
    "\n",
    "ax=sns.scatterplot(x='X', y='Y', hue=\"class\", \n",
    "                data=my_pos_df,palette=palette,s=20);\n",
    "\n",
    "#plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='black', marker='X', s=150, label=centroid_labels)\n",
    "for i,(center_x,center_y) in enumerate(centroids):\n",
    "    plt.text(center_x, center_y, f'Clu_{i}', fontsize=10,color='black', ha='center', va='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Over-representation pathway analysis\n",
    "#all the protein list, make it as the gene backgroud\n",
    "gene_name_list=[]\n",
    "for idx,i in enumerate(correlation_matrix.index):\n",
    "    gene_name=i.split(\".\")[0]\n",
    "    gene_name_list.append(gene_name)\n",
    "print(gene_name_list)\n",
    "\n",
    "#remove the duplicated from the gene backgroud\n",
    "gene_name_list_res = []\n",
    "[gene_name_list_res.append(x) for x in gene_name_list if x not in gene_name_list_res]\n",
    "glist_backgroud=gene_name_list_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate each class and apply pathway analysis\n",
    "#select the top1 pathway to annotate the cluster\n",
    "top_pathway=[]\n",
    "for i in range(20):\n",
    "    clu=my_pos_df[my_pos_df['class']==i]\n",
    "    \n",
    "    glist=[]\n",
    "    for i in clu.index:\n",
    "        glist.append(i.split(\".\")[0])\n",
    "    \n",
    "    enr = gp.enrichr(gene_list=glist, \n",
    "                 gene_sets=['WikiPathways_2019_Human'],\n",
    "                 #gene_sets=['KEGG_2021_Human'],\n",
    "                 background=glist_backgroud,\n",
    "                 organism='human', \n",
    "                 outdir=None, \n",
    "                )\n",
    "    top_pathway.append(enr.results.iloc[0,1])\n",
    "    print(enr.results.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert 2 lists to dictionary\n",
    "#key would be index, value would be pathway\n",
    "test_keys = range(20)\n",
    "test_values = top_pathway\n",
    "\n",
    "pathway_dic = {}\n",
    "for key in test_keys:\n",
    "    for value in test_values:\n",
    "        pathway_dic[key] = value\n",
    "        test_values.remove(value)\n",
    "        break\n",
    "pathway_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pos_df['class_2'] = my_pos_df['class'].map(pathway_dic)\n",
    "my_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=sns.scatterplot(x=\"X\", y=\"Y\", hue=\"class_2\", \n",
    "                data=my_pos_df, palette=palette, s=25);\n",
    "plt.legend(bbox_to_anchor=(1.02, 0.55), loc='upper left', borderaxespad=0)\n",
    "g.set(yticklabels=[])\n",
    "g.set(xticklabels=[])\n",
    "g.set(ylabel=None)\n",
    "g.set(xlabel=None)\n",
    "g.tick_params(left=False)\n",
    "g.tick_params(bottom=False)\n",
    "sns.despine(left=True, bottom=True, right=True)\n",
    "plt.savefig('V3_clustering_label.png',bbox_inches=\"tight\",dpi=800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
